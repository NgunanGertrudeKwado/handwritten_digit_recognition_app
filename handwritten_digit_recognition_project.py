# -*- coding: utf-8 -*-
"""Handwritten_Digit_Recognition_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fL7NfwevG9txI312Q8FZ-vsuN6INIhwa

# Data Collection
MNIST data set is available in the tensorflow and keras libraries
"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist

# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Check the shapes of training and testing sets
print(f"Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}")
print(f"Testing data shape: {X_test.shape}, Testing labels shape: {y_test.shape}")

# Visualize the first digit in the training data
plt.imshow(X_train[0], cmap='gray')
plt.title(f"Label: {y_train[0]}")
plt.show()

"""# Data Preprocessing
Normalizing pixel values to the range of 0 to 1 for better training performance.

also reshaping the data to include a channel dimension since we will be using Convolutional Neural Network (CNN)
"""

# Normalize the pixel values from 0-255 to 0-1
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape the data to add a channel dimension (required for CNN)
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)

# Print the new shape of the data
print(f"New training data shape: {X_train.shape}")
print(f"New testing data shape: {X_test.shape}")

"""# Exploratory Data Analysis (EDA)"""

import matplotlib.pyplot as plt

# Plot the first 10 training images
plt.figure(figsize=(10, 1))
for i in range(10):
    plt.subplot(1, 10, i+1)
    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')
    plt.axis('off')
    plt.title(y_train[i])
plt.show()

"""# Model Building and Training

"""

import tensorflow as tf
from tensorflow.keras import layers, models

# Define the CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')  # 10 classes for digits 0-9
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))

"""# Evaluating the Model"""

# Evaluate the model on the test dataset
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_accuracy:.4f}, Test loss: {test_loss:.4f}")

"""# Make Predictions"""

# Make predictions on the test dataset
predictions = model.predict(X_test)

# Convert predictions to class labels
predicted_classes = predictions.argmax(axis=-1)

# Print the first 10 predictions and their corresponding true labels
print("Predicted classes:", predicted_classes[:10])
print("True labels:", y_test[:10])

"""Observations

Perfect Accuracy on Sample Predictions:

The model predicted all ten test samples correctly, showing that it can recognize and classify these digits without error. This further supports the high accuracy observed in the earlier evaluation.

Model Confidence:

While this shows that the model is functioning well on these examples, it’s also important to evaluate its performance across the entire test dataset to ensure that it generalizes effectively and doesn’t just fit well to specific samples.

## Visualize Predictions
"""

import matplotlib.pyplot as plt

# Plot the first 10 test images along with predictions
plt.figure(figsize=(10, 1))
for i in range(10):
    plt.subplot(1, 10, i + 1)
    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')
    plt.axis('off')
    plt.title(predicted_classes[i])
plt.show()

"""This looks good.

Summary of Model Performance

Accurate Classification:
The model correctly identified the digits 7, 2, 1, 0, 4, 1, 4, 9, 5, and 9 from the test images, confirming its ability to generalize well to unseen data.

Model Reliability:
High accuracy across the test samples suggests that your model is reliable for digit recognition tasks.

# Confusion Matrix
Let's see how the model performs across all digit classes.
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Create confusion matrix
cm = confusion_matrix(y_test, predicted_classes)

# Visualize confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""Interpretation of Model Performance:

High Accuracy: Since most of the values are on the diagonal, the classifier performs well, making accurate predictions for most classes.

Confusions/Misclassifications:
Some misclassifications occur, especially between similar-looking classes (like predicting a 9 as a 4 or 8).

In summary, this confusion matrix indicates a strong model performance, but with room for improvement in certain misclassifications.
"""

from sklearn.metrics import classification_report

# Get a classification report
report = classification_report(y_test, predicted_classes)
print(report)

"""Understanding the Classification Report

Precision:

This measures the proportion of true positive predictions out of all positive predictions made. A high precision indicates that when the model predicts a class, it is usually correct.
For example, the precision for digit 7 is 0.97, meaning that 97% of the time the model predicted 7, it was correct.

Recall:

Also known as sensitivity, recall measures the proportion of true positives out of all actual instances of the class. A high recall indicates that the model can identify most of the actual instances.
For instance, the recall for digit 3 is 0.98, indicating that 98% of actual 3s were correctly identified by the model.

F1-Score:

The F1-score is the harmonic mean of precision and recall, providing a balance between the two metrics. A higher F1-score indicates better overall performance.
The F1-score for digit 5 is 0.98, suggesting a strong balance between precision and recall for this class.

Support:

This refers to the number of actual occurrences of each class in the specified dataset.
For example, there are 980 instances of digit 0 in the test set.

Accuracy:

Overall, the model achieved an accuracy of 0.99, meaning it correctly classified 99% of the 10,000 test samples.

Macro Avg and Weighted Avg:

Macro Average: This is the average performance across all classes, treating all classes equally.

Weighted Average: This takes into account the number of instances for each class, giving a more representative average, especially useful for imbalanced datasets.

Implications

Strong Performance: The model's performance is impressive, with all metrics above 0.97 across all classes, suggesting it is highly reliable for digit recognition.

Balanced Performance: The high recall and precision values indicate that the model is not only good at identifying digits but also minimizes false positives and negatives.

# Saving the model
"""

# Save the trained model
model.save('handwritten_digit_recognition_model.h5')

# Download the saved model file
from google.colab import files
files.download('handwritten_digit_recognition_model.h5')